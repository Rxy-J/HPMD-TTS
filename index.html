<!DOCTYPE html>
<html lang="en-US">

<head>
    <title>Towards Expressiveness Zero-Shot Speech Synthesis with Hierarchical Prosody Modeling</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
    <style>
                .method {
                        display: inline-block;
/*             width: 120px; /* Adjust the width as needed */ */
                        font-weight: bold;
                }

                .explanation {
                        display: inline-block;
/*             margin-left: 20px; /* Adjust the margin as needed */ */
                }
        </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    </section>
    <section class="main-content">
        <h1 id="">
            <center>Towards Expressiveness Zero-Shot Speech Synthesis with Hierarchical Prosody Modeling</center>
        </h1>
        <h3 id="">
            <center>A work submitted to INTERSPEECH 2024</center>
            <!-- <center>Dake Guo<sup>1</sup>, Xinfa Zhu<sup>1</sup>, Liumeng Xue<sup>2</sup>, Tao Li<sup>1</sup>, Yuanjun Lv<sup>1</sup>, Yuepeng Jiang<sup>1</sup>, Lei Xie<sup>1</sup></center> 
            <center><sup>1</sup>Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China </center>
            <center><sup>2</sup>School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China </center> -->
        </h3>


        <br><br>
        <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
        <p>Recent research in zero-shot speech synthesis has made significant progress in speaker similarity.
However, current efforts focus on timbre generalization rather than prosody modeling, which results in limited naturalness and expressiveness. To address this, we introduce a novel speech synthesis model trained on large-scale datasets, including both timbre and hierarchical prosody modeling.
As timbre is a global attribute closely linked to expressiveness, we adopt a global vector to model speaker identity while guiding prosody modeling.
Besides, given that prosody contains both global consistency and local variations, we introduce a diffusion model as the pitch predictor and employ a prosody adaptor to model prosody hierarchically, further enhancing the prosody quality of the synthesized speech.
Experimental results show that our model not only maintains comparable timbre quality to the baseline but also exhibits better naturalness and expressiveness. </p>

        <table frame=void rules=none>
            <tr>
                <center><img src='fig/overview.png' width="50%"></center>
                <center>Overview of our framework</center>
            </tr>
        </table>
        <br><br>


        <h2>2. Demos <a name="Comparison"></a></h2>
        <p>Methods</p>
        <ul>
            <li><b>VALL-E : </b>VALL-E<sup>1</sup> from <a href="https://speechresearch.github.io/naturalspeech3/">Amphion</a> toolkit</li> 
            <li><b>NS2 : </b>NaturalSpeech2<sup>2</sup> from Amphion toolkit</li>
        </ul>
        
        <h3>Speech</h3>
        <table>
            <tbody id="tbody_speech">
            </tbody>
        </table>

        <h3>Emotional Prompt</h3>
        <p>Text: 面色涨红，林肯猛然厉声大喝</p> 
        <table style="margin-left: auto; margin-right: auto;">
            <tbody id="tbody_emo_speech">
            </tbody>
        </table>
        
    <h3>References:</h3>
    <div>
        <!-- <div><cite><a href="https://arxiv.org/pdf/2307.16430.pdf">
                [1] J. Kong, J. Park, B. Kim, J. Kim, D. Kong, and S. Kim, “VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design,” in Proc. INTERSPEECH 2023, 2023, pp. 4374–4378.</a></cite></div> -->
        <!-- <div><cite><a href="https://arxiv.org/pdf/2211.02336.pdf">
                [2] Detai Xin, Sharath Adavanne, Federico Ang, Ashish Kulkarni, Shinnosuke Takamichi, and Hiroshi Saruwatari,
                "Improving speech prosody of audiobook text-to-speech synthesis with acoustic and textual contexts," in
                Proc. ICASSP, 2023, pp. 1–5.</a></cite></div>
        <div><cite><a href="https://arxiv.org/pdf/2203.12201.pdf">
                [3] Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Shiyin Kang, and Helen Meng, "Towards expressive speaking
                style modelling with hierarchical context information for Mandarin speech synthesis," in Proc. ICASSP
                2022, 2022, pp. 7922–7926.</a></cite></div> -->
    </div>


</html>

<script type="" text="javascript">
    function emo_speech() {
        let scenes = [["Prompt", "emo_refers"], ["Synthesised", "emo_samples"]]
        let emos = ["happy", "angry", "sad", "surprise", "fear", "disgust"]
        let short_data = `
        <tr>
            <td style="text-align: center; " rowspan=1><strong><strong></td>
        `
        for (const id in emos) {
            emo = emos[id]
            short_data += '<td style="text-align: center; width: 14%;" rowspan=1><strong>' + emo  + '<strong></td>'
        }
        short_data += `</tr>`
        for (let x in scenes) {
            let scene = scenes[x]
            let name = scene[0]
            let target_dir = scene[1]

            let scene_data = ""
            scene_data += '<tr>'
            scene_data += '<td style="text-align: center; width: 10%" rowspan=1>' + name + '</td>'
            for (let z in emos) {
                let emo = emos[z]
                scene_data += '<td style="text-align: center;"><audio style="width: 100%;" controls="" src="' + './demos/' + target_dir + '/' + emo + '.wav' + '"></audio></td>'
                
            }
            scene_data += '</tr>'
            short_data += scene_data
        }
        return short_data
    }

    function speech() {
        let scenes = [
            ["0", "都能看到人流中穿着深色制服的关员们在忙碌。"],
            ["1", "另一方面，要约束惩罚犯罪的权利本身。"],
            ["2", "我们每天谈论的这个政治。"],
            ["3", "哇，你什么时候变得这么厉害的。"],
            ["4", "她说仪式不仅得办，还得隆重地办，热烈地办。"],
            ["5", "她男朋友也是一天换一个的。"],
            ["6", "以后我们大概不会再见面了。"],
            ["7", "只要孩子不生病，好好学习，就心满意足了。他们根本不知道怎样才能跟孩子进行有效的交流。"],
            ["8", "哇，你什么时候变得这么厉害的。"],
            ["9", "今日，我们关注到了微博上有一些用户在散布关于刘强东先生的一些不实传言。特此声明如下： 刘强东先生在美国商务活动期间，遭遇到了失实指控，经过当地警方调查，未发现有任何不当行为，他将按照原计划继续其行程。我们将针对不实报道或造谣行为釆取必要的法律行动。"],
            ["10", "瞎说，我是出了名的眼力好。"],
            ["11", "那有什么问题的话，到时候咱们也可以方便交流，您看方便的话可以留一下您这边电话吗就是。"],
            ["12", "基尔德妈妈开始责怪他，并让他快滚。"],
        ]
        let models = [["Prompt", "ref"], ["Proposed", "any_speaker"],  ["VALL-E", "valle"], ["NS2", "ns2"]]
        let short_data = `
                <tr>
                    <td style="text-align: center; " rowspan=1><strong>Text<strong></td>
                `
        for (const id in models) {
            model = models[id][0]
            short_data += '<td style="text-align: center; width: 14%;" rowspan=1><strong>' + model  + '<strong></td>'
        }
        short_data += `</tr>`
        
        for (let x in scenes) {
            let scene = scenes[x]
            let file = scene[0]
            let text = scene[1]
            let scene_data = ""

            scene_data += '<tr>'
            scene_data += '<td style="text-align: center; width: 10%;" rowspan=1>' + text + '</td>'
            for (let z in models) {
                let model = models[z][1]
                scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './demos/' + model + '/' + file + '.wav' + '"></audio></td>'
                
            }
            scene_data += '</tr>'
            short_data += scene_data
        }
        return short_data
    }
    window.onload = function () {
        document.getElementById('tbody_speech').innerHTML = speech()
        document.getElementById('tbody_emo_speech').innerHTML = emo_speech()
    }
</script> 